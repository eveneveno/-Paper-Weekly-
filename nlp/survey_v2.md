`å¼‚æƒ³å¤©å¼€ 1.`
**What role are you playing, my little PIXEL?**
(top-down -> bottom-up)
**Self-supervised learning**
no pre-defined category
reasons about an image from pixel/patch/global level.
co-operation relation, supplementary, substitute, complement ...

(Inpainting => eliminate the task of ..., then it needs to look at other pixels for co-operation)
**æŠ±å›¢æ¸¸æˆ** (1,2,3 ... )
ä¸åŒçš„ä¸‹æ¸¸ä»»åŠ¡å¯¹åº”ä¸åŒçš„task
rich featureåªæ˜¯è¯´æ‡‚å¾—å¦‚ä½•æŒ‘é€‰ä»–çš„åˆä½œä¼™ä¼´
cam?

semantic segmentation (difference map)
scene graph generation (structure, how to extract knowledge)

Attention => via Kernel lens 
Attention => difference map 

`å¼‚æƒ³å¤©å¼€ 2.`
**Color Pallette**
How to sample color from palette, and how to locate them given a boundary.
é¢œè‰²å—ï¼ˆsymbolï¼‰ + è½®å»“ï¼ˆè¯­åºï¼‰

feature representation 
shape + color

å®Œå½¢å¡«ç©ºï¼Œå†™ä½œï¼Œæ˜¯äººç±»å¯¹è¯­è¨€æŒæ¡çš„æŠ€èƒ½ä½“ç°
é‚£ä¹ˆç»˜ç”»ï¼Œåˆ™æ˜¯äººä»¬å¯¹è§†è§‰ä¸–ç•Œçš„é‡æ„ã€‚
**ï¼ˆç´ æ/è‰²å½©ï¼‰**
1. reconstruction
2. (common) object feature å¦‚ä½•å»embed shape featureï¼Œä¸ºshapeæ„å»ºæ„ä¹‰
3. ImageNet å“ªäº›ç‰©ä½“æ˜¯ç”±é¢œè‰²å†³å®šçš„ å“ªäº›æ˜¯ç”±shapeå†³å®šçš„

SIMCLR: **color distortion**(è‰²å½©) + **cropping** (çŒœæµ‹ï¼šå¯èƒ½æ²¡æœ‰shapeä¿¡æ¯å¥½)


<br>


|  Paper | Venue | Remark
| ---------------- | ---- | ------------ | 
| [Image Transformer âœ¨](https://arxiv.org/pdf/1802.05751.pdf)| arxiv 2018 <br>(rejected by ICLR2018)| Cite [Conditional Image Generation with PixelCNN Decoders (NIPS'16)](https://arxiv.org/pdf/1606.05328.pdf) and [PixelSNAIL: An Improved Autoregressive Generative Model (ICML'18)](https://arxiv.org/pdf/1712.09763.pdf) <br>[[Code pytorch]](https://github.com/sahajgarg/image_transformer) å•å‘ => åŒå‘ => å››å‘
|[Contrastive Multiview Coding](https://arxiv.org/pdf/1906.05849.pdf) | arxiv 2020 (v2)| maximize Mutual Information
|[Conditional Image Generation with PixelCNN Decoders](https://arxiv.org/pdf/1606.05328.pdf)| NIPS 2016 |
| [Pixel Recurrent Neural Networks](https://arxiv.org/pdf/1601.06759.pdf)| 
|[Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles](https://arxiv.org/pdf/1603.09246.pdf) | ECCV 2016 | predict permutation order
| [Improving Generalization via Scalable Neighborhood Component Analysis](http://openaccess.thecvf.com/content_ECCV_2018/papers/Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper.pdf) | ECCV 2018 | 
| [PatchVAE: Learning Local Latent Codes for Recognition](https://arxiv.org/pdf/2004.03623.pdf) | arxiv 2020 |  withdraw(reject) from ICLR2020  
| [Steering Self-Supervised Feature Learning Beyond Local Pixel Statistics](https://arxiv.org/pdf/2004.02331.pdf) | arxiv 2020 | 
| [ğŸ’•ã€CPCã€‘Representation Learning with Contrastive Predictive Coding](https://arxiv.org/abs/1807.03748)|arxiv 2019 <br> (rejected by ICLR2020) | [`Talk`](https://slideslive.com/38922758/invited-talk-contrastive-predictive-coding) | 
| [The Right Tool for the Job: Matching Model and Instance Complexities](https://arxiv.org/pdf/2004.07453.pdf) | ACL 2020 [[code] (pytorch)](https://github.com/allenai/sledgehammer)| 
| [ğŸ’•A Simple Framework for Contrastive Learning of Visual Representations](https://arxiv.org/pdf/2002.05709.pdf) | arxiv 2020 [[code] (TPU+TF)](https://github.com/google-research/simclr) | We show that (1) **composition of data augmentations** plays a critical role in defining effective predictive tasks, (2) introducing **a learnable nonlinear transformation** between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from **larger batch sizes and more training steps** compared to supervised learning. <br> <img src="img/simclr.png" alt="drawing" width="300"/>
|[Learning Representations by Maximizing Mutual Information Across Views](https://papers.nips.cc/paper/9686-learning-representations-by-maximizing-mutual-information-across-views.pdf)| NIPS 2019 | 
| [Invariant Information Clustering for Unsupervised Image Classification and Segmentation](https://arxiv.org/pdf/1807.06653.pdf) | ICCV 2019 | 